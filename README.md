# MoCoGAN: Decomposing Motion and Content for Video Generation

This repository contains an implementation and further details of [MoCoGAN: Decomposing Motion and Content for Video Generation](https://www.google.com) by Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, Jan Kautz.

## Representation

MoCoGAN is a generative model for videos, which generates videos from random inputs. It features separated representations of motion and content, offering control over what is generated. For example, MoCoGAN can generate the same object performing different actions, as well as the same action performed by different objects

![MoCoGAN Representation](https://github.com/sergeytulyakov/mocogan/raw/master/doc/controlling-content-and-motion.png)

## Generated Videos

All videos in this section are generated by MoCoGAN.

We trained MoCoGAN on a synthetically generated dataset of moving shapes. The color, shape and size of each moving shape represent content. Action is a specific motion direction. The shapes move bottom-top and right-left along a random Bezier curve.

![Shape motion](https://github.com/sergeytulyakov/mocogan/raw/master/doc/shapes.gif "Shape motion")

We trained MoCoGAN on a [human action dataset](http://www.wisdom.weizmann.ac.il/~vision/SpaceTimeActions.html) where content is represented by the performer, executing several actions:

![Human actions](https://github.com/sergeytulyakov/mocogan/raw/master/doc/action.gif "Human actions")

We then train on the [MUG Facial Expression Database](https://mug.ee.auth.gr/fed/) to generate facial expressions. Each column has fixed identity, each row shows the same action:

![Facial expressions](https://github.com/sergeytulyakov/mocogan/raw/master/doc/faces.gif "Facial expressions")

<!---
We have collected a large-scale TaiChi dataset including 4.5K videos of TaiChi performers. Below are videos generated by MoCoGAN.

![TaiChi](https://github.com/sergeytulyakov/mocogan/raw/master/doc/ours_taichi.gif "TaiChi")
--->

## Code

The code will be made available soon
